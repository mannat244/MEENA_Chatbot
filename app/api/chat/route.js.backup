import { Groq } from 'groq-sdk';
import { SarvamAIClient } from 'sarvamai';
import { NextResponse }     // üîç DEBUG: Log the complete conversation being sent to LLM
    console.log('\nüîç ===== COMPLETE LLM REQUEST DEBUG =====');
    console.log('ü§ñ Model:', model);
    console.log('üåê Language:', language);
    console.log('üîß Has Context:', hasContext);
    console.log('üìù Message Length:', message?.length || 0);
    console.log('üìÑ Context Length:', contextualInfo?.length || 0);
    console.log('üí¨ History Messages:', conversationMessages.length);ext/server';

// Initialize Groq client
const groq = new Groq({
  apiKey: process.env.GROQ_API_KEY,
  dangerouslyAllowBrowser: false
});

// Initialize SarvamAI client (only if API key is available)
const sarvam = process.env.SARVAM_API_KEY ? new SarvamAIClient({ 
  apiSubscriptionKey: process.env.SARVAM_API_KEY 
}) : null;

export async function POST(request) {
  try {
    const { 
      message, 
      originalMessage,
      language, 
      contextualInfo, 
      hasContext,
      model: requestedModel = 'gemma2-9b-it',
      conversationHistory = [] 
    } = await request.json();
    
    // Validate and set the model
    const model = validateModel(requestedModel);
    
    console.log('\nü§ñ ===== CHAT API RECEIVED REQUEST =====');
    console.log('üìù Original Message:', originalMessage || 'Not provided');
    console.log('üìã Enhanced Message Length:', message?.length || 0);
    console.log('üîß Has Context:', hasContext);
    console.log('üìä Context Info Length:', contextualInfo?.length || 0);
    console.log('üí¨ Conversation History:', conversationHistory?.length || 0, 'messages');

    if (!message) {
      return NextResponse.json(
        { error: 'Message is required' },
        { status: 400 }
      );
    }

    // Language instruction based on selected language
    const languageInstruction = language !== 'English' 
      ? `Please respond in ${language}. ` 
      : '';

    // Build conversation history for context
    const conversationMessages = [];
    
    // Add conversation history (last 6 messages for context)
    if (conversationHistory && conversationHistory.length > 0) {
      const recentHistory = conversationHistory.slice(-6); // Last 6 messages
      recentHistory.forEach(msg => {
        if (msg.sender === 'user') {
          conversationMessages.push({ role: 'user', content: msg.text });
        } else if (msg.sender === 'meena') {
          conversationMessages.push({ role: 'assistant', content: msg.text });
        }
      });
      
      console.log('\nüí¨ ===== CONVERSATION HISTORY DEBUG =====');
      console.log('üìä Total history messages:', conversationMessages.length);
      conversationMessages.forEach((msg, i) => {
        console.log(`${i + 1}. ${msg.role}: ${msg.content.substring(0, 100)}...`);
      });
      console.log('‚úÖ Conversation context will be sent to LLM for follow-up understanding');
    } else {
      console.log('\n‚ùå No conversation history - this is a fresh conversation');
    }

    // System prompt for MEENA - Updated to work with context and conversation history
    const basePrompt = `You are MEENA, a Multilingual Embeddable Educational Natural Language Assistant for Maulana Azad National Institute of Technology (MANIT), Bhopal. You help students with academic queries, campus information, and administrative processes.

GUIDELINES:
- Communicate naturally in English, Hindi, Punjabi, Marathi, Tamil, Telugu and other Indian regional languages
- Keep replies concise, accurate, and conversational
- Maintain a professional, student-friendly tone
- Always be helpful and provide practical guidance
- Use previous conversation context to provide relevant follow-up responses
- Reference earlier parts of the conversation when appropriate

${hasContext ? `CONTEXT-BASED RESPONSE MODE:
- The user's message includes relevant information from the MANIT knowledge base
- Prioritize and use the provided knowledge base information in your response
- Cite specific details from the knowledge base when answering
- If the knowledge base information answers their question, use it confidently
- If additional verification is needed, suggest they contact the appropriate office` : `NO-CONTEXT RESPONSE MODE:
- No specific MANIT information is available for this query
- Provide general guidance and suggest they contact MANIT offices for accurate details
- Mention that for current information, they should check the official MANIT website or contact:
  * Academic Office: For exam dates, academic calendar
  * Accounts Office: For fee information
  * Hostel Office: For accommodation queries
  * Training & Placement: For placement related queries`}

${languageInstruction}`;

    const systemPrompt = basePrompt;

    // üîç DEBUG: Log the complete conversation being sent to LLM
    console.log('\nüîç ===== COMPLETE LLM REQUEST DEBUG =====');
    console.log('ü§ñ Model:', model);
    console.log('üåê Language:', language);
    console.log('ÔøΩ Has Context:', hasContext);
    console.log('ÔøΩ Message Length:', message?.length || 0);
    console.log('üìÑ Context Length:', contextualInfo?.length || 0);
    
    console.log('\nüì§ SYSTEM PROMPT BEING SENT TO LLM:');
    console.log('‚ïê'.repeat(80));
    console.log(systemPrompt);
    console.log('‚ïê'.repeat(80));
    
    console.log('\nüë§ USER MESSAGE BEING SENT TO LLM:');
    console.log('‚îÄ'.repeat(80));
    console.log(message);
    console.log('‚îÄ'.repeat(80));
    
    if (hasContext) {
      console.log('‚úÖ CONTEXT EMBEDDED IN USER MESSAGE - MEENA will see the knowledge base info directly');
    } else {
      console.log('‚ùå NO CONTEXT - MEENA will give general response');
    }

    // Get model configuration
    const modelConfig = AVAILABLE_MODELS[model];
    const provider = modelConfig?.provider || 'groq';
    
    console.log(`ü§ñ Using ${provider.toUpperCase()} with model: ${model}`);

    // Create chat completion based on provider
    let chatCompletion;
    
    if (provider === 'sarvam') {
      // Check if SarvamAI is available
      if (!sarvam) {
        throw new Error('SarvamAI API key not configured. Please add SARVAM_API_KEY to your environment variables.');
      }
      
      // SarvamAI API call - NOTE: SarvamAI doesn't support streaming in the same way as Groq
      // We'll get the complete response and simulate streaming
      const sarvamMessages = [
        {
          role: "system",
          content: systemPrompt
        },
        ...conversationMessages, // Add conversation history
        {
          role: "user",
          content: message
        }
      ];
      
      chatCompletion = await sarvam.chat.completions({
        messages: sarvamMessages,
        model: model,
        temperature: 0.7,
        max_tokens: 1024,
        stream: false, // SarvamAI client doesn't support streaming the same way
        wiki_grounding: hasContext ? false : true, // Use wiki grounding when no context available
        top_p: 0.9
      });
    } else {
      // Groq API call (default)
      const groqMessages = [
        {
          role: "system",
          content: systemPrompt
        },
        ...conversationMessages, // Add conversation history
        {
          role: "user",
          content: message
        }
      ];
      
      chatCompletion = await groq.chat.completions.create({
        messages: groqMessages,
        model: model,
        temperature: 1,
        max_completion_tokens: 1024,
        top_p: 1,
        stream: true,
        stop: null
      });
    }

    // Create a readable stream for the response
    const encoder = new TextEncoder();
    const stream = new ReadableStream({
      async start(controller) {
        try {
          if (provider === 'sarvam') {
            // Handle SarvamAI non-streaming response - simulate streaming by sending chunks progressively
            const fullResponse = chatCompletion.choices?.[0]?.message?.content || '';
            console.log('SarvamAI full response received:', fullResponse.length, 'characters');
            
            if (fullResponse) {
              // Split into words and stream them progressively for better UX
              const words = fullResponse.split(' ');
              
              for (let i = 0; i < words.length; i++) {
                const wordToSend = (i > 0 ? ' ' : '') + words[i];
                const data = `data: ${JSON.stringify({ content: wordToSend })}\n\n`;
                controller.enqueue(encoder.encode(data));
                
                // Small delay between words to simulate streaming
                await new Promise(resolve => setTimeout(resolve, 30));
              }
            } else {
              // Fallback if no content
              const errorContent = 'Sorry, I could not generate a response. Please try again.';
              const data = `data: ${JSON.stringify({ content: errorContent })}\n\n`;
              controller.enqueue(encoder.encode(data));
            }
          } else {
            // Handle Groq streaming response
            for await (const chunk of chatCompletion) {
              const content = chunk.choices[0]?.delta?.content || '';
              if (content) {
                const data = `data: ${JSON.stringify({ content })}\n\n`;
                controller.enqueue(encoder.encode(data));
              }
            }
          }
          
          // Send completion signal
          const doneData = `data: ${JSON.stringify({ done: true })}\n\n`;
          controller.enqueue(encoder.encode(doneData));
          controller.close();
          
        } catch (error) {
          console.error(`${provider.toUpperCase()} Streaming error:`, error);
          const errorData = `data: ${JSON.stringify({ error: `${provider.toUpperCase()} streaming failed: ${error.message}` })}\n\n`;
          controller.enqueue(encoder.encode(errorData));
          controller.close();
        }
      }
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Transfer-Encoding': 'chunked',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive'
      }
    });

  } catch (error) {
    console.error('Chat API error:', error);
    return NextResponse.json(
      { error: 'Failed to process chat request', details: error.message },
      { status: 500 }
    );
  }
}

// Available models configuration
const AVAILABLE_MODELS = {
  // SarvamAI Models (Primary - supports English + Indian Languages)
  'sarvam-m': { provider: 'sarvam', name: 'Sarvam-M', category: 'Primary' },
  
  // Groq Models (Alternative - Gemma only)
  'gemma2-9b-it': { provider: 'groq', name: 'Gemma 2 9B', category: 'Alternative' },
};

// Validate model selection
function validateModel(model) {
  return AVAILABLE_MODELS[model] ? model : 'sarvam-m';
}

// Check if API keys are configured
function isProviderAvailable(provider) {
  switch (provider) {
    case 'groq':
      return !!process.env.GROQ_API_KEY;
    case 'sarvam':
      return !!process.env.SARVAM_API_KEY;
    default:
      return false;
  }
}

// GET endpoint for testing and model info
export async function GET() {
  return NextResponse.json({ 
    message: 'MEENA Chat API is running',
    version: '3.0.0',
    providers: {
      sarvam: {
        available: isProviderAvailable('sarvam'),
        models: ['sarvam-m'],
        specialties: ['English', 'Hindi', 'Tamil', 'Telugu', 'Bengali', 'Gujarati', 'Marathi', 'Kannada', 'Multilingual Support']
      },
      groq: {
        available: isProviderAvailable('groq'),
        models: ['gemma2-9b-it'],
        note: 'Alternative model option'
      }
    },
    models: Object.entries(AVAILABLE_MODELS).map(([key, value]) => ({
      id: key,
      provider: value.provider,
      name: value.name,
      category: value.category,
      available: isProviderAvailable(value.provider),
      recommended: value.category === 'Primary' ? 'Best for all languages including English & Indian languages' : 'Alternative general purpose model'
    })),
    timestamp: new Date().toISOString()
  });
}
